name: 2.Test Harness Execution

on:
  workflow_dispatch:
    inputs:
      run_k8s:
        description: 'Run test harness against K8s (requires KUBECONFIG secret)'
        type: boolean
        default: true
  pull_request:
    branches:
      - '**' # Trigger on pull requests to any branch
  push:
    branches:
      - main # Trigger on main only

jobs:
  test-harness:
    if: github.event_name != 'pull_request' || github.event.pull_request.head.repo.full_name == github.repository
    runs-on: ubuntu-latest
    concurrency:
      group: test-harness-${{ github.head_ref || github.ref_name }}
      cancel-in-progress: true
    permissions:
      contents: read
    env:
      TEST_HARNESS_PATH: ./testharness
      HARNESS_MODE: k8s
      KUBECONFIG: ${{ secrets.OVH_KUBECONFIG }}
      K8S_INGRESS_DOMAIN: testnet.verana.network
      DH_USERNAME: ${{ secrets.DOCKER_HUB_LOGIN }}
      DH_TOKEN: ${{ secrets.DOCKER_HUB_PWD }}
      IMAGE_REPO: ${{ secrets.DOCKER_HUB_LOGIN }}/verana-node
      VALIDATOR_COUNT: "2"

    steps:
      - name: Checkout this repository
        uses: actions/checkout@v3
        with:
          fetch-depth: 0
          ref: ${{ github.head_ref || github.ref_name }}

      - name: Determine harness mode
        run: |
          if [ "$HARNESS_MODE" = "k8s" ]; then
            if [ "${GITHUB_EVENT_NAME}" = "workflow_dispatch" ] && [ "${{ inputs.run_k8s }}" != "true" ]; then
              echo "HARNESS_MODE_EFFECTIVE=docker" >> $GITHUB_ENV
              echo "K8S disabled via workflow_dispatch input; falling back to docker." >> $GITHUB_STEP_SUMMARY
            elif [ -n "${KUBECONFIG:-}" ]; then
              echo "HARNESS_MODE_EFFECTIVE=k8s" >> $GITHUB_ENV
            else
              echo "Missing KUBECONFIG secret; cannot run K8S harness." >> $GITHUB_STEP_SUMMARY
              exit 1
            fi
          else
            echo "HARNESS_MODE_EFFECTIVE=docker" >> $GITHUB_ENV
          fi
          if [ "${GITHUB_EVENT_NAME}" = "pull_request" ]; then
            PR_NUMBER=$(echo "$GITHUB_REF" | awk -F'/' '{print $3}')
            echo "IMAGE_TAG=${IMAGE_REPO}:pr-${PR_NUMBER}" >> $GITHUB_ENV
            BRANCH_REF="${GITHUB_HEAD_REF:-${GITHUB_REF_NAME:-$GITHUB_REF}}"
          elif [ "${GITHUB_EVENT_NAME}" = "workflow_dispatch" ]; then
            BRANCH_REF="${GITHUB_REF_NAME:-$GITHUB_REF}"
            export BRANCH_REF
            PR_NUMBER="$(python3 scripts/ci/resolve_pr_number.py)"
            if [ -n "${PR_NUMBER}" ]; then
              echo "IMAGE_TAG=${IMAGE_REPO}:pr-${PR_NUMBER}" >> $GITHUB_ENV
              echo "Resolved PR #${PR_NUMBER} for ${BRANCH_REF}; using PR image tag." >> $GITHUB_STEP_SUMMARY
            else
              BRANCH_TAG="${BRANCH_REF//\//-}"
              SHORT_SHA="$(echo "${GITHUB_SHA}" | cut -c1-7)"
              echo "IMAGE_TAG=${IMAGE_REPO}:sha-${SHORT_SHA}" >> $GITHUB_ENV
              echo "No open PR for ${BRANCH_REF}; using sha tag ${SHORT_SHA}." >> $GITHUB_STEP_SUMMARY
            fi
          else
            BRANCH_REF="${GITHUB_REF_NAME:-$GITHUB_REF}"
            BRANCH_TAG="${BRANCH_REF//\//-}"
            SHORT_SHA="$(echo "${GITHUB_SHA}" | cut -c1-7)"
            echo "IMAGE_TAG=${IMAGE_REPO}:sha-${SHORT_SHA}" >> $GITHUB_ENV
          fi
          BRANCH_REF="${BRANCH_REF:-${GITHUB_REF_NAME:-$GITHUB_REF}}"
          BRANCH_TAG="${BRANCH_REF//\//-}"
          if [ -n "${PR_NUMBER:-}" ]; then
            echo "NAMESPACE=vna-devnet-${BRANCH_TAG}-pr-${PR_NUMBER}" >> $GITHUB_ENV
          elif [ "${BRANCH_TAG}" = "main" ]; then
            echo "NAMESPACE=vna-devnet-main" >> $GITHUB_ENV
          else
            echo "NAMESPACE=vna-devnet-${BRANCH_TAG}" >> $GITHUB_ENV
          fi

      - name: Login to Docker Hub
        if: env.HARNESS_MODE_EFFECTIVE == 'docker'
        uses: docker/login-action@v3
        with:
          username: ${{ env.DH_USERNAME }}
          password: ${{ env.DH_TOKEN }}

      - name: Build canonical image
        if: env.HARNESS_MODE_EFFECTIVE == 'docker'
        run: |
          echo "Pulling $IMAGE_TAG"
          attempts=0
          max_attempts=30
          until docker pull "$IMAGE_TAG"; do
            attempts=$((attempts + 1))
            if [ "$attempts" -ge "$max_attempts" ]; then
              echo "Failed to pull $IMAGE_TAG after $attempts attempts." >&2
              exit 1
            fi
            echo "Image not ready yet, retrying in 20s... ($attempts/$max_attempts)"
            sleep 20
          done
          docker tag "$IMAGE_TAG" verana-node:ci

      - name: Set up kubectl
        if: env.HARNESS_MODE_EFFECTIVE == 'k8s'
        uses: azure/setup-kubectl@v4

      - name: Configure kubeconfig
        if: env.HARNESS_MODE_EFFECTIVE == 'k8s'
        run: |
          echo "$KUBECONFIG" > k8s_config
          echo "KUBECONFIG=$(pwd)/k8s_config" >> $GITHUB_ENV

      - name: Deploy single-node testnet to K8s
        if: env.HARNESS_MODE_EFFECTIVE == 'k8s'
        run: |
          export KUBECONFIG=$KUBECONFIG
          echo "Checking image availability: $IMAGE_TAG"
          attempts=0
          max_attempts=30
          until docker pull "$IMAGE_TAG"; do
            attempts=$((attempts + 1))
            if [ "$attempts" -ge "$max_attempts" ]; then
              echo "Image $IMAGE_TAG not available after $attempts attempts." >&2
              exit 1
            fi
            echo "Image not ready yet, retrying in 20s... ($attempts/$max_attempts)"
            sleep 20
          done
          echo "Installing veranad from $IMAGE_TAG for local test harness use"
          container_id="$(docker create "$IMAGE_TAG")"
          docker cp "${container_id}:/usr/local/bin/veranad" /usr/local/bin/veranad
          docker rm "$container_id"
          chmod +x /usr/local/bin/veranad
          if kubectl get namespace "$NAMESPACE" >/dev/null 2>&1; then
            deletion_ts="$(kubectl get namespace "$NAMESPACE" -o jsonpath='{.metadata.deletionTimestamp}')"
            if [ -n "$deletion_ts" ]; then
              echo "Namespace $NAMESPACE is terminating; waiting for deletion before recreating..."
              if ! kubectl wait --for=delete "namespace/$NAMESPACE" --timeout=300s; then
                echo "Namespace $NAMESPACE is still terminating after 300s." >&2
                kubectl get namespace "$NAMESPACE" -o yaml || true
                exit 1
              fi
            else
              echo "Reusing existing namespace $NAMESPACE"
            fi
          fi
          if ! kubectl get namespace "$NAMESPACE" >/dev/null 2>&1; then
            kubectl create namespace "$NAMESPACE"
          fi
          kubectl -n "$NAMESPACE" create configmap verana-testharness-scripts \
            --from-file=scripts/setup_primary_validator.sh \
            --from-file=scripts/setup_secondary_validator.sh \
            --dry-run=client -o yaml | kubectl apply -f -

          cat > /tmp/verana-testharness-deploy.yaml <<EOF
          apiVersion: apps/v1
          kind: Deployment
          metadata:
            name: verana-testharness-node
          spec:
            replicas: 1
            selector:
              matchLabels:
                app: verana-testharness
            template:
              metadata:
                labels:
                  app: verana-testharness
                annotations:
                  ci.verana/run-id: "${GITHUB_RUN_ID}-${GITHUB_RUN_ATTEMPT}"
              spec:
                containers:
                - name: veranad
                  image: ${IMAGE_TAG}
                  imagePullPolicy: Always
                  command: ["/bin/bash","-lc"]
                  args:
                    - |
                      apt-get update && apt-get install -y python3 && rm -rf /var/lib/apt/lists/*
                      cp /scripts/setup_primary_validator.sh /tmp/setup_primary_validator.sh
                      chmod +x /tmp/setup_primary_validator.sh
                      /tmp/setup_primary_validator.sh
                  ports:
                    - containerPort: 26656
                      name: p2p
                    - containerPort: 26657
                      name: rpc
                    - containerPort: 1317
                      name: api
                    - containerPort: 9090
                      name: grpc
                  volumeMounts:
                    - name: scripts
                      mountPath: /scripts
                volumes:
                  - name: scripts
                    configMap:
                      name: verana-testharness-scripts
          EOF

          cat > /tmp/verana-testharness-svc.yaml <<EOF
          apiVersion: v1
          kind: Service
          metadata:
            name: verana-testharness
          spec:
            selector:
              app: verana-testharness
            ports:
              - name: rpc
                port: 26657
                targetPort: 26657
              - name: api
                port: 1317
                targetPort: 1317
          EOF

          cat > /tmp/verana-testharness-ingress.yaml <<EOF
          apiVersion: networking.k8s.io/v1
          kind: Ingress
          metadata:
            name: verana-testharness
          spec:
            rules:
              - host: rpc-${NAMESPACE}.${K8S_INGRESS_DOMAIN}
                http:
                  paths:
                    - path: /
                      pathType: Prefix
                      backend:
                        service:
                          name: verana-testharness
                          port:
                            number: 26657
              - host: api-${NAMESPACE}.${K8S_INGRESS_DOMAIN}
                http:
                  paths:
                    - path: /
                      pathType: Prefix
                      backend:
                        service:
                          name: verana-testharness
                          port:
                            number: 1317
          EOF

          kubectl apply -n "$NAMESPACE" -f /tmp/verana-testharness-deploy.yaml
          kubectl apply -n "$NAMESPACE" -f /tmp/verana-testharness-svc.yaml
          kubectl apply -n "$NAMESPACE" -f /tmp/verana-testharness-ingress.yaml

          if ! kubectl -n "$NAMESPACE" rollout status deployment/verana-testharness-node --timeout=300s; then
            kubectl -n "$NAMESPACE" get pods -o wide || true
            kubectl -n "$NAMESPACE" describe pod -l app=verana-testharness || true
            kubectl -n "$NAMESPACE" logs -l app=verana-testharness --all-containers --tail=200 || true
            exit 1
          fi
          POD_NAME="$(kubectl -n "$NAMESPACE" get pods -l app=verana-testharness --sort-by=.metadata.creationTimestamp -o name | tail -n1 | cut -d/ -f2)"
          if [ -z "$POD_NAME" ]; then
            echo "No verana-testharness pod found after rollout." >&2
            kubectl -n "$NAMESPACE" get pods -o wide || true
            exit 1
          fi
          if ! kubectl -n "$NAMESPACE" wait --for=condition=Ready "pod/$POD_NAME" --timeout=300s; then
            echo "Pod $POD_NAME did not become Ready." >&2
            kubectl -n "$NAMESPACE" get pods -o wide || true
            kubectl -n "$NAMESPACE" describe "pod/$POD_NAME" || true
            kubectl -n "$NAMESPACE" logs "pod/$POD_NAME" --all-containers --tail=200 || true
            exit 1
          fi
          for i in {1..120}; do
            if kubectl -n "$NAMESPACE" exec "$POD_NAME" -- curl -s http://127.0.0.1:26657/status >/dev/null 2>&1; then
              echo "RPC is ready inside pod"
              break
            fi
            sleep 2
          done
          if ! kubectl -n "$NAMESPACE" exec "$POD_NAME" -- curl -s http://127.0.0.1:26657/status >/dev/null 2>&1; then
            echo "RPC did not become ready inside pod." >&2
            kubectl -n "$NAMESPACE" get pods -o wide || true
            kubectl -n "$NAMESPACE" describe pod -l app=verana-testharness || true
            kubectl -n "$NAMESPACE" logs -l app=verana-testharness --all-containers --tail=200 || true
            exit 1
          fi

      - name: Run test harness in canonical image
        if: env.HARNESS_MODE_EFFECTIVE == 'docker'
        run: |
          docker run --rm \
            -v "$GITHUB_WORKSPACE:/workspace" \
            -w /workspace \
            -e TEST_HARNESS_PATH \
            -e VALIDATOR_COUNT \
            verana-node:ci \
            /bin/bash -lc 'set -e
              if ! command -v go >/dev/null 2>&1; then
                GO_VERSION="1.25.0"
                curl -fsSL "https://go.dev/dl/go${GO_VERSION}.linux-amd64.tar.gz" -o /tmp/go.tgz
                rm -rf /usr/local/go
                tar -C /usr/local -xzf /tmp/go.tgz
                rm -f /tmp/go.tgz
              fi
              export PATH="/usr/local/go/bin:$PATH"
              echo ./scripts/setup_primary_validator.sh....
              ./scripts/setup_primary_validator.sh &
              PID=$!
              echo "Asynchronous setup process started with PID: $PID"
              for i in {1..120}; do
                if curl -s http://127.0.0.1:26657/status >/dev/null 2>&1; then
                  echo "Primary validator RPC is ready"
                  break
                fi
                sleep 2
              done
              if ! curl -s http://127.0.0.1:26657/status >/dev/null 2>&1; then
                echo "Primary validator RPC did not become ready in docker mode." >&2
                exit 1
              fi
              if [ "${VALIDATOR_COUNT:-1}" -ge 2 ]; then
                echo "Bootstrapping secondary validator in docker mode..."
                ./scripts/setup_secondary_validator.sh 2
              fi
              cd $TEST_HARNESS_PATH
              ./scripts/setup_accounts.sh
              ./scripts/run_all.sh
              kill -9 $PID || true
            '

      - name: Run test harness against K8s
        if: env.HARNESS_MODE_EFFECTIVE == 'k8s'
        run: |
          export KUBECONFIG=$KUBECONFIG
          POD_NAME="$(kubectl -n "$NAMESPACE" get pods -l app=verana-testharness --sort-by=.metadata.creationTimestamp -o name | tail -n1 | cut -d/ -f2)"
          if [ -z "$POD_NAME" ]; then
            echo "No verana-testharness pod found before port-forward." >&2
            kubectl -n "$NAMESPACE" get pods -o wide || true
            exit 1
          fi
          PORT_FORWARD_PID=""
          start_port_forward() {
            : > /tmp/portforward.log
            kubectl -n "$NAMESPACE" port-forward "pod/$POD_NAME" 26657:26657 1317:1317 >> /tmp/portforward.log 2>&1 &
            PORT_FORWARD_PID=$!
          }
          stop_port_forward() {
            if [ -n "${PORT_FORWARD_PID:-}" ] && kill -0 "$PORT_FORWARD_PID" >/dev/null 2>&1; then
              kill "$PORT_FORWARD_PID" >/dev/null 2>&1 || true
              wait "$PORT_FORWARD_PID" >/dev/null 2>&1 || true
            fi
          }
          trap 'stop_port_forward' EXIT

          start_port_forward
          for i in {1..120}; do
            if ! kill -0 "$PORT_FORWARD_PID" >/dev/null 2>&1; then
              echo "Port-forward process exited; restarting (attempt $i/120)"
              tail -n 50 /tmp/portforward.log || true
              start_port_forward
              sleep 1
            fi
            if curl -s http://127.0.0.1:26657/status >/dev/null 2>&1; then
              echo "RPC is ready"
              break
            fi
            sleep 2
          done
          if ! curl -s http://127.0.0.1:26657/status >/dev/null 2>&1; then
            echo "RPC did not become ready via port-forward." >&2
            cat /tmp/portforward.log || true
            kubectl -n "$NAMESPACE" get pods -o wide || true
            kubectl -n "$NAMESPACE" describe pod -l app=verana-testharness || true
            kubectl -n "$NAMESPACE" logs -l app=verana-testharness --all-containers --tail=200 || true
            exit 1
          fi
          if [ "${VALIDATOR_COUNT:-1}" -ge 2 ]; then
            echo "Bootstrapping secondary validator in K8s pod..."
            if ! kubectl -n "$NAMESPACE" exec "$POD_NAME" -- /bin/bash -lc '
              cp /scripts/setup_secondary_validator.sh /tmp/setup_secondary_validator.sh
              chmod +x /tmp/setup_secondary_validator.sh
              /tmp/setup_secondary_validator.sh 2
            '; then
              echo "Secondary validator bootstrap failed." >&2
              kubectl -n "$NAMESPACE" exec "$POD_NAME" -- /bin/bash -lc 'tail -n 200 /tmp/verana-validator2.log || true'
              exit 1
            fi
          fi
          echo "Running test harness against K8s endpoints"
          echo "K8S_RPC_ENDPOINT=http://127.0.0.1:26657"
          echo "K8S_LCD_ENDPOINT=http://127.0.0.1:1317"
          cd ${{ env.TEST_HARNESS_PATH }}
          export NODE_RPC="http://127.0.0.1:26657"
          ./scripts/setup_accounts.sh
          ./scripts/run_all.sh

      - name: Finalize without error
        run: echo "Test harness execution completed successfully."

      - name: Cleanup K8s namespace (workflow_dispatch only)
        if: always() && env.HARNESS_MODE_EFFECTIVE == 'k8s' && github.event_name == 'workflow_dispatch'
        run: |
          export KUBECONFIG=$KUBECONFIG
          echo "Cleaning up namespace $NAMESPACE"
          kubectl delete namespace "$NAMESPACE" --wait=false || true
